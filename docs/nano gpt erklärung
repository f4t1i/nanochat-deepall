Technischer Bericht: Die Reproduktion von GPT-2 (124M) – Eine vollständige Analyse der Methodik, Architektur und Implementierung nach Andrej Karpathy
Die Entwicklung von Large Language Models (LLMs) hat in den letzten Jahren eine beispiellose Beschleunigung erfahren, wobei die Veröffentlichung von GPT-2 durch OpenAI im Jahr 2019 einen entscheidenden Wendepunkt markierte. In seinem umfassenden Tutorial „Let's reproduce GPT-2 (124M)“ demonstriert Andrej Karpathy, wie dieses historisch bedeutsame Modell mit moderner Hardware und optimierten Software-Frameworks in einem Bruchteil der ursprünglichen Zeit und Kosten rekonstruiert werden kann. Während das Training des ursprünglichen Modells im Jahr 2019 erhebliche Rechenressourcen und Zeit erforderte, ist die Reproduktion heute auf einem modernen Cloud-GPU-Knoten in etwa einer Stunde für ca. 10 USD möglich. Dieser Bericht liefert eine tiefgehende Analyse der von Karpathy vorgestellten Schritte, beginnend bei der leeren Datei bis hin zu einem voll funktionsfähigen Modell, das die Leistungsfähigkeit des ursprünglichen GPT-2 erreicht oder sogar übertrifft.   

Architekturelle Grundlagen und strukturelle Spezifikationen
Die Architektur von GPT-2 basiert auf dem Transformer-Modell, speziell einer Decoder-only-Variante, die für die autoregressive Sprachmodellierung optimiert ist. Das Ziel des Modells besteht darin, die Wahrscheinlichkeitsverteilung des nächsten Tokens in einer gegebenen Sequenz vorherzusagen. Karpathy betont, dass die Struktur des GPT-2-Modells weitgehend identisch mit der des späteren GPT-3-Modells ist, weshalb das Verständnis dieser Architektur ein grundlegendes Wissen über den aktuellen Stand der Technik (State of the Art) vermittelt.   

Konfiguration des 124M-Modells
Das Modell mit 124 Millionen Parametern bildet die kleinste Variante der GPT-2-Familie. Die präzise Einhaltung der Hyperparameter ist entscheidend für die Kompatibilität mit den von OpenAI bereitgestellten Gewichten. Die folgende Tabelle fasst die Kernspezifikationen zusammen:   

Parameter	Wert	Beschreibung
Anzahl der Schichten (n 
layer
​
 )	12	
Die vertikale Tiefe der Transformer-Blöcke.

Einbettungsdimension (n 
embd
​
 )	768	
Die Breite der Vektorrepräsentationen.

Aufmerksamkeit-Köpfe (n 
head
​
 )	12	
Die Anzahl der parallelen Self-Attention-Einheiten.

Kontextlänge (block_size)	1024	
Maximale Sequenzlänge für die Eingabe.

Vokabulargröße (vocab_size)	50257	
Anzahl der eindeutigen Tokens im BPE-Vokabular.

  
Diese Konfiguration führt zu einer Gesamtanzahl von etwa 124 Millionen Parametern, wobei ein signifikanter Teil auf die Einbettungsschichten entfällt.   

Der Transformer-Block: Eine detaillierte Analyse
Jeder der 12 Transformer-Blöcke besteht aus zwei Hauptsubschichten: der Multi-Head Causal Self-Attention und dem Feed-Forward Network (MLP). Ein entscheidendes Merkmal der GPT-2-Architektur im Vergleich zum ursprünglichen Transformer-Papier ist die Position der Schichtnormierung (Layer Normalization). GPT-2 verwendet die „Pre-Normalization“-Konfiguration, bei der die Normierung vor der jeweiligen Subschicht erfolgt, ergänzt durch eine finale Normierung vor dem Modell-Head.   

Die mathematische Abfolge innerhalb eines Blocks lässt sich wie folgt beschreiben:

x=x+Attention(LayerNorm 
1
​
 (x))
x=x+MLP(LayerNorm 
2
​
 (x))
Dieser Aufbau nutzt Residualverbindungen (Skip-Connections), um den Gradientenfluss während des Backpropagation-Prozesses zu stabilisieren und das Training tieferer Netzwerke zu ermöglichen.   

Die Multi-Head Attention-Komponente
Innerhalb der Self-Attention-Schicht interagieren die Tokens miteinander, um Informationen über die gesamte Sequenz hinweg zu aggregieren. Karpathy beschreibt diesen Prozess als einen Mechanismus, bei dem Tokens „kommunizieren“. Er verwendet die Analogie von Map-Reduce, wobei Attention die „Reduce“-Operation (Aggregation von Informationen) und das MLP die „Map“-Operation (lokale Verarbeitung jedes Tokens) darstellt.   

Technisch gesehen werden für jedes Token drei Vektoren erzeugt: Query (Q), Key (K) und Value (V). Die Aufmerksamkeit berechnet sich durch das Skalarprodukt von Query und Key, gefolgt von einer Skalierung und einer Softmax-Aktivierung:

Attention(Q,K,V)=softmax( 
d 
k
​
 

​
 
QK 
T
 
​
 +M)V
Hierbei steht M für die kausale Maske, die sicherstellt, dass ein Token nur auf vorangegangene Tokens (und sich selbst) achten kann, um die autoregressive Eigenschaft zu wahren.   

Das Multi-Layer Perceptron (MLP) und die GELU-Aktivierung
Das MLP dient der Transformation der aggregierten Informationen auf Token-Ebene. Es besteht aus einer Expansion der Dimension von 768 auf 3072, gefolgt von einer Aktivierung und einer Projektion zurück auf 768. Ein interessantes historisches Detail, das Karpathy hervorhebt, ist die Verwendung der Gaussian Error Linear Unit (GELU) in ihrer approximierten Form. OpenAI implementierte 2019 eine Annäherung, da die exakte Berechnung in TensorFlow damals rechenintensiv war:   

GELU(x)≈0.5x(1+tanh[ 
π
2
​
 

​
 (x+0.044715x 
3
 )])
Für eine exakte Reproduktion der Gewichte muss diese Approximationsformel auch in der modernen PyTorch-Implementierung verwendet werden, obwohl heutige Hardware die exakte Funktion effizient berechnen könnte.   

Datenvorbereitung und Tokenisierung
Ein wesentlicher Bestandteil des Erfolgs von GPT-2 ist die Qualität und Quantität der Trainingsdaten. Karpathy verwendet für die Reproduktion den FineWeb-Edu-Datensatz, eine hochgradig kuratierte und gefilterte Version des Common Crawl-Datensatzes, die speziell für pädagogische Inhalte optimiert wurde.   

Der FineWeb-Edu-Datensatz
Der Datensatz wird in Form von Shards verarbeitet, um die Speichereffizienz zu maximieren. Ein Shard besteht typischerweise aus etwa 100 Millionen Tokens, was bei einer Gesamtmenge von 10 Milliarden Tokens zu 100 Shards führt. Diese Sharding-Strategie ermöglicht es dem Modell, Daten kontinuierlich von der Festplatte zu streamen, ohne den Arbeitsspeicher zu überlasten.   

Komponente	Details
Datensatzquelle	
HuggingFaceFW/fineweb-edu 

Tokenisierungs-Tool	
tiktoken (GPT-2 Encoding) 

Shard-Größe	
100 Millionen Tokens (ca. 100 MB pro Shard) 

Datentyp	
uint16 (für Tokens bis 50.257) 

  
Der Tokenisierungsprozess (Byte-Pair Encoding)
GPT-2 verwendet Byte-Pair Encoding (BPE), um Text in eine Sequenz von Zahlen zu übersetzen. BPE ist ein Kompromiss zwischen zeichenbasierten und wortbasierten Modellen. Es beginnt mit einzelnen Bytes und fusioniert iterativ die am häufigsten vorkommenden Paare zu neuen Tokens. Dies ermöglicht es dem Modell, ein festes Vokabular von 50.257 Tokens zu pflegen, das dennoch jedes beliebige Wort der englischen Sprache (und darüber hinaus) darstellen kann. Karpathy erklärt, dass die Effizienz des Tokenizers entscheidend ist, da er die Sequenzlänge verkürzt und somit die Rechenlast im Transformer reduziert.   

Implementierung und Gewichtsübertragung (Weight Porting)
Ein zentraler Schritt in Karpathys Video ist die Validierung der Architektur durch das Laden der originalen Gewichte von Hugging Face. Dies stellt sicher, dass die selbst geschriebene PyTorch-Klasse exakt so funktioniert wie das Originalmodell von OpenAI.   

Mapping der Parameter-Keys
Da die ursprüngliche Implementierung von OpenAI in TensorFlow verfasst wurde, unterscheiden sich die Namen der Variablen von den typischen PyTorch-Konventionen. Beispielsweise werden die kombinierten Gewichte für Query, Key und Value in der Aufmerksamkeitsschicht als c_attn bezeichnet, was in PyTorch einer nn.Linear-Schicht entspricht. Ein kritischer Aspekt beim Laden dieser Gewichte ist die Transponierung: Während TensorFlow-Gewichte oft in einer transponierten Form vorliegen, erwartet PyTorch ein anderes Layout.   

Weight Tying Strategie
Karpathy implementiert das sogenannte „Weight Tying“ zwischen der Token-Einbettungsschicht und der finalen linearen Schicht (LM Head). Da diese beiden Schichten semantisch ähnliche Operationen ausführen (Zahlen in Vektoren umwandeln und umgekehrt), können sie dieselben Gewichte teilen. Dies reduziert die Anzahl der trainierbaren Parameter um etwa 40 Millionen, ohne die Leistung negativ zu beeinflussen. Das Modell lernt so konsistentere Repräsentationen für Tokens in der Eingabe- und Ausgabephase.   

Leistungsoptimierung und Trainingsbeschleunigung
Einer der beeindruckendsten Teile der Demonstration ist die Steigerung der Trainingsgeschwindigkeit durch moderne Softwaretechniken. Von einer Basisgeschwindigkeit von etwa 16.000 Tokens pro Sekunde optimiert Karpathy das System auf über 1,4 Millionen Tokens pro Sekunde in einer Multi-GPU-Umgebung.   

Präzisionstuning: TF32 und BF16
Die Verwendung von standardmäßigen 32-Bit-Fließkommazahlen (FP32) ist für das Training von Deep-Learning-Modellen oft unnötig präzise und rechenintensiv.

TensorFloat-32 (TF32): Auf modernen NVIDIA-GPUs (Ampere-Architektur und neuer) können Matrixmultiplikationen beschleunigt werden, indem die Mantisse von 23 auf 10 Bit gekürzt wird, während der Exponent von 8 Bit beibehalten wird. Dies führt zu einer Beschleunigung um den Faktor 3 bis 8 bei minimalem Genauigkeitsverlust. Die Aktivierung erfolgt über torch.set_float32_matmul_precision('high').   

Bfloat16 (BF16): Dieses Format bietet den gleichen Dynamikbereich wie FP32, verbraucht aber nur 16 Bit Speicher. Im Gegensatz zu FP16 benötigt BF16 kein Gradient Scaling, was die Stabilität des Trainings erhöht. Karpathy nutzt torch.autocast, um Berechnungen automatisch im effizienteren BF16-Format auszuführen.   

Kernel-Fusion mit torch.compile
PyTorch 2.0 führte torch.compile ein, ein Feature, das den Python-Code analysiert und optimierte CUDA-Kernel generiert. Durch das Zusammenfassen (Fusion) mehrerer Operationen in einen einzigen GPU-Kernel wird der Datentransfer zwischen dem globalen Speicher und den Recheneinheiten minimiert. In Karpathys Test führte dies zu einer Reduktion der Schrittzeit von 300 ms auf 130 ms. Er merkt jedoch an, dass torch.compile zum Zeitpunkt der Videoaufnahme Probleme mit der Generierung und bestimmten Evaluierungsfunktionen hatte.   

Flash Attention
Die Aufmerksamkeit-Matrix wächst quadratisch mit der Sequenzlänge, was normalerweise zu einem Speicherengpass führt. Flash Attention ist eine hocheffiziente Implementierung, die die Aufmerksamkeit blockweise berechnet, ohne die vollständige Matrix jemals im Speicher zu materialisieren. Dies spart nicht nur VRAM, sondern beschleunigt den Prozess durch eine bessere Ausnutzung der Speicherbandbreite erheblich. Der Durchsatz stieg in der Demonstration von 126.000 auf 170.000 Tokens pro Sekunde allein durch diesen Schritt.   

Optimierungsschritt	Durchsatz (Tokens/Sek)	Impact
Baseline (FP32)	~16.000	
Referenzwert 

TF32 Aktivierung	~49.000	
3x schneller 

BF16 Autocast	~54.000	
Geringer Zusatzgewinn 

torch.compile	~126.000	
Massiver Gewinn durch Kernel-Fusion 

Flash Attention	~170.000	
Höchste Effizienz in der Attention 

  
Das Trainingsregime: Hyperparameter und Optimierung
Das Training eines Modells wie GPT-2 erfordert eine sorgfältige Abstimmung der Lernrate und der Regularisierung. Da die ursprüngliche GPT-2-Veröffentlichung nicht alle Details preisgab, orientiert sich Karpathy an den Parametern aus der GPT-3-Publikation.   

Lernraten-Zeitplan (Learning Rate Schedule)
Ein Cosinus-Decay-Zeitplan mit einer linearen Warmup-Phase wird verwendet. Die Lernrate steigt über die ersten Schritte (z.B. 375 Schritte) auf einen Maximalwert (z.B. 6×10 
−4
 ) an und sinkt dann allmählich auf etwa 10% dieses Wertes ab. Dies stellt sicher, dass das Modell zu Beginn stabil lernt und gegen Ende des Trainings fein abgestimmt wird.   

Weight Decay und Gradient Clipping
Um eine Überanpassung (Overfitting) zu vermeiden, wird AdamW als Optimierer eingesetzt. Hierbei wird ein Weight Decay von 0,1 auf alle Gewichte angewendet, die an Matrixmultiplikationen beteiligt sind (2D-Tensoren). Biases und Parameter der Layer-Normierung sind davon ausgeschlossen. Zusätzlich wird die Norm der Gradienten auf 1,0 begrenzt (Gradient Clipping), um das Phänomen explodierender Gradienten zu verhindern, das bei tiefen Netzwerken oft auftritt.   

Initialisierungsskalierung für Residualpfade
Karpathy implementiert eine spezielle Skalierung für die Gewichte in den Residualpfaden. Da jede Schicht zur Varianz des Signals beiträgt, werden die Gewichte der Projektionsschichten innerhalb der Blöcke mit dem Faktor 1/ 
2×n 
layer
​
 

​
  skaliert. Diese Maßnahme ist entscheidend, um die Signalstärke über alle 12 Schichten hinweg im Gleichgewicht zu halten und die Stabilität des Trainings zu gewährleisten.   

Evaluierung und Benchmark-Ergebnisse
Um den Erfolg der Reproduktion zu messen, wird das Modell nicht nur gegen einen Validierungsdatensatz getestet, sondern auch gegen etablierte Benchmarks wie HellaSwag geprüft.   

HellaSwag-Evaluierung
HellaSwag misst die Fähigkeit eines Modells, eine Alltagssituation logisch korrekt zu vervollständigen. Das Modell muss aus vier Antwortmöglichkeiten die wahrscheinlichste auswählen. Die Implementierung erfolgt durch den Vergleich der durchschnittlichen Log-Likelihood jeder Antwortmöglichkeit. Das Modell von Karpathy erreicht nach einem vollständigen Trainingslauf Werte um 32%, was fast identisch mit der Leistung des originalen GPT-2 124M-Modells ist.   

Modellgenerierung
Nach dem Training zeigt das Modell die Fähigkeit, kohärente englische Sätze zu generieren. Während es anfangs (nach ca. 10 Milliarden Tokens) noch einfache Fehler macht, verbessert sich die Qualität mit zunehmendem Training (z.B. 40 Milliarden Tokens) deutlich. Karpathy merkt an, dass diese Modelle „träumen“ und Internetdokumente halluzinieren, da sie noch kein spezifisches Chat-Finetuning erhalten haben.   

Vergleich: PyTorch vs. llm.c
Neben der Python-basierten Implementierung stellt Karpathy llm.c vor – eine in reinem C und CUDA geschriebene Version des GPT-2-Trainings.   

Effizienz: llm.c ist tendenziell schneller beim Start und pro Schritt, da es den Overhead des Python-Interpreters und der PyTorch-Abstraktionen vermeidet. In direkten Vergleichen erreichte llm.c etwa 223.000 Tokens/Sekunde gegenüber 185.000 Tokens/Sekunde in der PyTorch-Version.   

Wartbarkeit: Während PyTorch für Experimente und schnelle Iterationen ideal ist, dient llm.c als Beweis dafür, wie kompakt und effizient der Kernalgorithmus eines Transformers in hardwarenahen Sprachen implementiert werden kann.   

Praktische Anleitung zur Reproduktion
Für Anwender, die das Modell selbst trainieren möchten, bietet das Repository eine klare Struktur und Skripte wie speedrun.sh.   

Hardware-Auswahl: Ein 8xH100 GPU-Knoten wird empfohlen, um das Modell in etwa 4 Stunden für ca. 100 USD zu trainieren.   

Umgebungseinrichtung: Klonen des Repositories build-nanogpt und Installation der Abhängigkeiten mit dem uv-Manager.   

Datenbeschaffung: Ausführen von python fineweb.py, um die Shards des FineWeb-Edu-Datensatzes lokal zu speichern.   

Training: Start des Trainings mit torchrun --standalone --nproc_per_node=8 train_gpt2.py. Die Ergebnisse werden in Log-Dateien und durch regelmäßige Checkpoints dokumentiert.   

Inferenz: Verwendung eines Chat-CLI-Skripts oder eines einfachen Web-UIs, um mit dem fertig trainierten Modell zu interagieren.   

Zusammenfassung und Ausblick
Die Reproduktion von GPT-2 (124M) nach der Methodik von Andrej Karpathy ist ein Beweis für die Reife der modernen KI-Entwicklungstools. Was 2019 eine industrielle Großleistung war, ist heute ein lehrreiches Projekt für Einzelpersonen. Die Kombination aus hocheffizienten Datensätzen wie FineWeb-Edu, algorithmischen Durchbrüchen wie Flash Attention und Hardware-Features wie TF32 ermöglicht eine Demokratisierung der Modellentwicklung.   

Die tiefgehende Analyse zeigt, dass der Transformer-Algorithmus im Kern elegant und skalierbar ist. Die Prinzipien, die Karpathy am 124M-Modell demonstriert – von der Gewichtsinitialisierung bis zur Präzisionssteuerung –, bilden das Fundament für die heutigen Giganten der KI-Welt wie GPT-4 und darüber hinaus. Für die Zukunft ist zu erwarten, dass die Grenzen zwischen Software-Abstraktion und Hardware-Optimierung weiter verschwimmen werden, um noch effizientere und leistungsfähigere Sprachmodelle zu ermöglichen.   


github.com
build-nanogpt/README.md at master - GitHub
Wird in einem neuen Fenster geöffnet

github.com
Video+code lecture on building nanoGPT from scratch - GitHub
Wird in einem neuen Fenster geöffnet

reddit.com
reproduce GPT-2 (124M) from scratch, by Andrej Karpathy : r/learnmachinelearning - Reddit
Wird in einem neuen Fenster geöffnet

towardsdatascience.com
Line By Line, Let's Reproduce GPT-2: Section 1 | Towards Data Science
Wird in einem neuen Fenster geöffnet

wandb.ai
Notes on Implementing GPT-2 from scratch | gpt2-sai – Weights & Biases - Wandb
Wird in einem neuen Fenster geöffnet

reddit.com
"Let's reproduce GPT-2 (124M)"- from GOAT Andrej Karpathy : r/LocalLLaMA - Reddit
Wird in einem neuen Fenster geöffnet

medium.com
Build Your Own ChatGPT in an Afternoon: The NanoGPT Guide | by NeuralNikitha - Medium
Wird in einem neuen Fenster geöffnet

youtube.com
Let's reproduce GPT-2 (124M)
Wird in einem neuen Fenster geöffnet

medium.com
Let's reproduce GPT-2, again. Recently, I've had had the chance to ...
Wird in einem neuen Fenster geöffnet

gilesthomas.com
Writing an LLM from scratch, part 28 -- training a base model from scratch on an RTX 3090
Wird in einem neuen Fenster geöffnet

medium.com
Decoding Karpathy's GPT-2 Training: Key Takeaways | by Jaideep Ray | Better ML | Medium
Wird in einem neuen Fenster geöffnet

news.ycombinator.com
karpathy/build-nanogpt: Video + code lecture on building nanoGPT from scratch | Hacker News
Wird in einem neuen Fenster geöffnet

github.com
build-nanogpt/train_gpt2.py at master · karpathy/build-nanogpt ...
Wird in einem neuen Fenster geöffnet

analyticsvidhya.com
Build ChatGPT Clone with Andrej Karpathy's nanochat - Analytics Vidhya
Wird in einem neuen Fenster geöffnet

github.com
fineweb.py - karpathy/build-nanogpt - GitHub
Wird in einem neuen Fenster geöffnet

medium.com
Deep Dive into LLMs. summary of Andrej Karpathy LLM tutorial | by Ravikiran Gunale
Wird in einem neuen Fenster geöffnet

fast.ai
Let's Build the GPT Tokenizer: A Complete Guide to Tokenization in LLMs - Fast.ai
Wird in einem neuen Fenster geöffnet

kaggle.com
Andrej Karpathy: Let's build GPT: from scratch, in code, spelled out. | Kaggle
Wird in einem neuen Fenster geöffnet

reddit.com
Reproducing GPT-2 (124M) from scratch - results & notes : r/LocalLLaMA - Reddit
Wird in einem neuen Fenster geöffnet

towardsdatascience.com
Line-By-Line, Let's Reproduce GPT-2: Section 3 - Training | Towards Data Science
Wird in einem neuen Fenster geöffnet

github.com
GPT-2 (124M) reproduction time discrepancy · karpathy build-nanogpt · Discussion #75
Wird in einem neuen Fenster geöffnet

github.com
Andrej Karpathy - GitHub
Wird in einem neuen Fenster geöffnet

github.com
karpathy/nanochat: The best ChatGPT that $100 can buy. - GitHub
Wird in einem neuen Fenster geöffnet
